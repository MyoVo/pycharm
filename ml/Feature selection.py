import matplotlib.pyplot as plt

# Data for XGBoost classifier
xgb_errors = [
    0.5236541598694944, 0.5154975530179445, 0.399673735725938,
    0.2773246329526917, 0.20391517128874392, 0.19902120717781402,
    0.1908646003262643, 0.13539967373572592, 0.10114192495921692,
    0.1076672104404568, 0.11978792822185971, 0.1176672104404568,
    0.11603588907014683, 0.11929853181076676
]

# Data for other classifiers can be added in similar lists
rf_errors = [
    0.5122349102773247, 0.5106035889070146, 0.3752039151712887,
    0.2920065252854812, 0.21370309951060362, 0.22185970636215335,
    0.23001631321370308, 0.15986949429037522, 0.1426231647634586,
    0.17050570962479613, 0.1681646003262646, 0.1587438825448616,
    0.17561174551386622, 0.15419249592169656
]
svm_errors = [
    0.46492659053833607, 0.4584013050570962, 0.3474714518760196,
    0.2724306688417618, 0.19738988580750405, 0.18923327895595432,
    0.18923327895595432, 0.12398042414355626, 0.10303588907014683,
    0.10929853181076676, 0.10277324632952689, 0.11440456769983686,
    0.15277324632952689, 0.16114192495921692
]
dndt_errors = [
    0.47471451876019577, 0.46492659053833607, 0.35889070146818924,
    0.29526916802610115, 0.234910277324633, 0.25611745513866235,
    0.25611745513866235, 0.1272430668841762, 0.102234910277324629,
    0.12887438825448616, 0.10277324632952689, 0.12561174551386622,
    0.1114192495921692, 0.1419249592169656
]
elm_errors = [
    0.4584013050570962, 0.47960848287112556, 0.38662316476345837,
    0.26590538336052205, 0.19249592169657426, 0.17618270799347469,
    0.17781402936378465, 0.12398042414355626, 0.11561174551386622,
    0.12561174551386622, 0.12561174551386622, 0.11419249592169656,
    0.13539967373572592, 0.12561174551386622
]
tabnet_errors = [
    0.7161500815660685, 0.5138662316476346, 0.435562805872757,
    0.36378466557911904, 0.37357259380097885, 0.2724306688417618,
    0.23001631321370308, 0.20717781402936375, 0.11092985318107662,
    0.15497553017944532, 0.17944535073409462, 0.15497553017944532,
    0.12398042414355626, 0.13050570962479613
]

# Number of features
features = ['P4', 'min', 'std', 'P3', 'P1', 'max', 'P13', 'P24', 'P2', 'P12', 'P14', 'mean', 'P34', 'P23']
num_features = list(range(1, 15))

# Plotting the graph
plt.figure(figsize=(12, 8))

# Plotting each classifier's error rates
plt.plot(features, xgb_errors, marker='o', label='XGBoost')
plt.plot(features, rf_errors, marker='x', label='Random Forest')
plt.plot(features, svm_errors, marker='s', label='SVM')
plt.plot(features, dndt_errors, marker='^', label='DNDT')
plt.plot(features, elm_errors, marker='v', label='ELM')
plt.plot(features, tabnet_errors, marker='d', label='TabNet')

# Adding labels and title
plt.xlabel('Number of Features')
plt.ylabel('Classification Error')
plt.title('Classification Error vs. Number of Features Added')
plt.legend()
plt.grid(True)
plt.xticks(features, rotation=45)
# 设置x轴标签为45度
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(f'huxinxi.svg', format='svg', bbox_inches='tight')
plt.show()
